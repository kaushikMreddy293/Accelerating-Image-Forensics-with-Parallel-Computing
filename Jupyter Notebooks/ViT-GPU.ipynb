{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96febea5",
   "metadata": {},
   "source": [
    "# Serial Execution of Vision Transformer on 1 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9ba02",
   "metadata": {},
   "source": [
    "## Run the code cells below \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f6311",
   "metadata": {},
   "source": [
    "## What this code does\n",
    "- Loads data using Pytorch loader\n",
    "- Trains Transformer on 1 GPU\n",
    "- Trains Transformer on 2 GPUs using Data parallel\n",
    "- Tested both on P100 GPU\n",
    "- The Dataparallel was done but not used for analysis, just learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750a07d4-687d-4b93-87d6-719025749116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malikireddy.k/.local/lib/python3.12/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import ViTFeatureExtractor\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Constants\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 62\n",
    "IMG_SIZE = 224  # Required for ViT\n",
    "SUBSET_SIZE = 12000  # 10k train + 2k test\n",
    "\n",
    "# Dataset paths\n",
    "DATASET_DIR = \"dataset\"\n",
    "CSV_PATH = os.path.join(DATASET_DIR, \"train.csv\")\n",
    "IMAGE_DIR = os.path.join(DATASET_DIR, \"train_data\")\n",
    "\n",
    "\n",
    "# Shuffle full dataset of 70k and split manually\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "\n",
    "# Feature extractor for ViT\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "])\n",
    "\n",
    "# Custom Dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rel_path = self.df.loc[idx, 'file_name']\n",
    "        img_path = os.path.join(self.img_dir, os.path.basename(rel_path))\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = int(self.df.loc[idx, 'label'])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "\n",
    "# Split 10k train + 2k val + 2k test\n",
    "train_df = df.iloc[:40000].reset_index(drop=True)\n",
    "val_df = df.iloc[40000:45000].reset_index(drop=True)\n",
    "test_df = df.iloc[45000:50000].reset_index(drop=True)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImageDataset(train_df, IMAGE_DIR, transform)\n",
    "val_dataset = ImageDataset(val_df, IMAGE_DIR, transform)\n",
    "test_dataset = ImageDataset(test_df, IMAGE_DIR, transform)\n",
    "\n",
    "# Create loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ebae5e-59fc-40e2-88f3-d90b71236898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images).logits\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ecff6c1-633b-4bbe-b61d-ef29af55f329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 started\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 52\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mreset_peak_memory_stats(device)\n\u001b[0;32m---> 52\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m     53\u001b[0m acc \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, device)\n\u001b[1;32m     55\u001b[0m mem_allocated \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(device) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 11\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(2):  # change as needed\n",
    "    print(f\"\\nEpoch {epoch + 1} started\")\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    acc = evaluate(model, test_loader, device)\n",
    "\n",
    "    mem_allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "    mem_reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)\n",
    "    peak_mem_allocated = torch.cuda.max_memory_allocated(device) / (1024 ** 2)\n",
    "    peak_mem_reserved = torch.cuda.max_memory_reserved(device) / (1024 ** 2)\n",
    "\n",
    "    print(f\"[GPU 1] Epoch {epoch + 1}, Loss: {train_loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "    print(f\"[GPU 1] Memory Allocated: {mem_allocated:.2f} MB\")\n",
    "    print(f\"[GPU 1] Memory Reserved: {mem_reserved:.2f} MB\")\n",
    "    print(f\"[GPU 1] Peak Memory Allocated: {peak_mem_allocated:.2f} MB\")\n",
    "    print(f\"[GPU 1] Peak Memory Reserved: {peak_mem_reserved:.2f} MB\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal training time on 1 GPU: {(end_time - start_time):.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab7c83-ddc2-49e7-a098-1dc4806efcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_with_metrics(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images).logits\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"Real\", \"AI\"])\n",
    "    return acc, cm, report\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, title=\"Confusion Matrix\"):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Real\", \"AI\"], yticklabels=[\"Real\", \"AI\"])\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b3194-18d5-471d-b29c-e4fc79022478",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc, val_cm, val_report = evaluate_with_metrics(model, val_loader, device)\n",
    "print(\"\\n[GPU 1] Validation Accuracy:\", val_acc)\n",
    "print(\"[GPU 1] Confusion Matrix:\\n\", val_cm)\n",
    "print(\"[GPU 1] Classification Report:\\n\", val_report)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(val_cm, title=\"[GPU 1] Validation Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c38eb59-bbf5-4b1e-ba19-4823d0435384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "# Use DataParallel for multi-GPU if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "print(\"Starting 2-GPU Training\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(2):  # change as needed\n",
    "    print(f\"\\nEpoch {epoch + 1} started\")\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    acc = evaluate(model, test_loader, device)\n",
    "\n",
    "    mem_allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "    mem_reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)\n",
    "    peak_mem_allocated = torch.cuda.max_memory_allocated(device) / (1024 ** 2)\n",
    "    peak_mem_reserved = torch.cuda.max_memory_reserved(device) / (1024 ** 2)\n",
    "\n",
    "    print(f\"[GPU 2] Epoch {epoch + 1}, Loss: {train_loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "    print(f\"[GPU 2] Memory Allocated: {mem_allocated:.2f} MB\")\n",
    "    print(f\"[GPU 2] Memory Reserved: {mem_reserved:.2f} MB\")\n",
    "    print(f\"[GPU 2] Peak Memory Allocated: {peak_mem_allocated:.2f} MB\")\n",
    "    print(f\"[GPU 2] Peak Memory Reserved: {peak_mem_reserved:.2f} MB\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal training time on 2 GPUs: {(end_time - start_time):.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8728850c-590e-47b8-8eee-7528d9edffc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d2383-f42a-43a4-b6e9-5d95fce74713",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        'google/vit-base-patch16-224',\n",
    "        num_labels=2,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(2):  # change as needed\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        print(f\"[GPU 2] Epoch {epoch+1}, Loss: {train_loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Total training time on 2 GPUs: {(end_time - start_time):.2f} seconds\")\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_acc, val_cm, val_report = evaluate_with_metrics(model, val_loader, device)\n",
    "    print(\"\\n[GPU 2] Validation Accuracy:\", val_acc)\n",
    "    print(\"[GPU 2] Confusion Matrix:\\n\", val_cm)\n",
    "    print(\"[GPU 2] Classification Report:\\n\", val_report)\n",
    "\n",
    "    plot_confusion_matrix(val_cm, title=\"[GPU X] Validation Confusion Matrix\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716f114-91bc-49cf-9b86-fdc15b9ded21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
