#  Accelerating Image Forensics With Parallel Computing

## Overview

This project explores the application of advanced deep learning and parallel computing strategies to accelerate **image forensics**, focusing on detecting **AI-generated (deepfake) images**. Two core architectures â€” **ResNet18** and **Vision Transformers (ViT)** â€” were used to classify real vs. AI-generated content, while various parallelism techniques were benchmarked for performance and scalability.

---

## ðŸ”Key Highlights

-  ðŸ§  **Models**: ResNet18 (lightweight CNN) & Vision Transformer (ViT)
-  ðŸ“ **Dataset**: 90,000+ real and AI-generated images from [Kaggle](https://www.kaggle.com/competitions/detect-ai-vs-human-generated-images)
-  âš™ï¸ **Parallelism Explored**:
    - Distributed Data Parallel (DDP) across 1, 2, and 4 GPUs
    - Model Parallelism (manual split across GPUs)
    - Mixed Precision Training (AMP with FP16)
    - CPU Thread Parallelism (1, 2, 4 workers)
-  ðŸ“Š **Benchmarking**:
    - Measured speedup, efficiency, memory usage, training time, accuracy
    - Comparative visualizations across CPU/GPU setups

---

## ðŸ§ª Methodology

1. **Preprocessing**:
   - Resized images to 224Ã—224
   - Normalized using ImageNet statistics
   - Loaded via PyTorch `DataLoader` with multiprocessing
2. **Training**:
   - ResNet18 and ViT trained using `torchrun` or model-split logic
   - All benchmarks tested with and without AMP
   - CPU parallelism used for baseline speed comparison
3. **Evaluation**:
   - Accuracy measured on test/validation sets
   - GPU memory tracked via `torch.cuda.max_memory_allocated()`
   - Speedup = Baseline Time / Current Time
   - Efficiency = Speedup / Number of Devices

---

## ðŸ“ˆ Results Summary

| Method                | Model     | GPUs | Speedup | Accuracy | Training Time (s) | Max GPU Mem (GB) |
|----------------------|-----------|------|---------|----------|--------------------|------------------|
| Single GPU (Baseline)| ResNet18  | 1    | 1Ã—      | 98.58%   | 743.45             | 0.50             |
| Model Parallel        | ResNet18  | 2    | ~1Ã—     | 98.75%   | 741.73             | 0.46             |
| DDP + AMP             | ViT       | 4    | 2.25Ã—   | 99.90%   | 254.92             | ~0.40            |
| CPU Parallel (2 Core) | ResNet18  | CPU  | 1.42Ã—   | 96.42%   | (Scaled from 25%)  | â€”                |

>  ViT benefited greatly from GPU scaling. ResNet18 showed limited GPU scaling due to its small model size but benefited from memory-aware training.

---

## ðŸ’» Streamlit App

Experience our model live through a user-friendly UI:

 
ðŸ”— [Launch App](https://ai-vs-real-image-detection-hpc.streamlit.app/)

**Features**:
- Upload real/AI images for classification
- Compare ResNet18 vs ViT predictions
- Download logs and CSV outputs
- Batch processing + confidence scores

---

## ðŸ“˜ Full Report & Code

This repository includes:
-  Performance plots (training time, speedup, efficiency)
-  Model training/evaluation code (CPU & GPU)
-  Full project report with graphs and conclusions

---

##  Acknowledgements

- **CSYE7105 Instructor (High Performance ML & AI)** â€“ Professor Handan Liu
- **Teammate** â€“ Aditi Deodhar
- Built with: PyTorch, Streamlit, OpenCV, NCCL, DDP

---

##  Future Scope

- Integrate **Fully Sharded Data Parallel (FSDP)**
- Extend to **video-based deepfake detection**


## References

- Tidio AI Detection Blog: [Tidio Blog](https://www.tidio.com/blog/ai-test/)
- Human vs AI-Generated Images: [arXiv:2412.09715](https://arxiv.org/abs/2412.09715)
- PyTorch Documentation: [PyTorch.org](https://pytorch.org/docs/stable/index.html)
- Vision Transformers Article: [Vision Transformers - Wolfe](https://cameronrwolfe.substack.com/p/vision-transformers)
- ResNet Deepfake Detection: [Springer](https://link.springer.com/article/10.1007/s00371-024-03613-x)


