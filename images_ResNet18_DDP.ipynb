{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a97a5d-6840-4d6c-abd0-d7d62fd4f7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_resnet_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_resnet_train.py\n",
    " \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    " \n",
    "# --- Dataset ---\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, root_dir='dataset'):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row['file_name'])\n",
    "        label = int(row['label'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    " \n",
    "# --- DDP Training Function ---\n",
    "def main():\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(\"cuda\", local_rank)\n",
    " \n",
    "    # Transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    " \n",
    "    # # Load full dataset\n",
    "    # dataset = TrainDataset(\"dataset/train.csv\", transform=transform)\n",
    "    # total_len = len(full_dataset)\n",
    "    # half_len = total_len // 2\n",
    "    # subset_indices = random.sample(range(total_len), half_len)\n",
    "    # sampler = DistributedSampler(dataset)\n",
    "    # dataloader = DataLoader(dataset, batch_size=64, sampler=sampler, num_workers=2)\n",
    "\n",
    "    # Load full dataset\n",
    "    full_dataset = TrainDataset(\"dataset/train.csv\", transform=transform)\n",
    "    \n",
    "    # Use only 10% of the dataset\n",
    "    total_len = len(full_dataset)\n",
    "    ten_percent_len = int(0.1 * total_len)\n",
    "    subset_indices = random.sample(range(total_len), ten_percent_len)\n",
    "    \n",
    "    # Create the subset\n",
    "    subset_dataset = Subset(full_dataset, subset_indices)\n",
    "    \n",
    "    # Create DistributedSampler on subset\n",
    "    sampler = DistributedSampler(subset_dataset)\n",
    "    \n",
    "    # Dataloader on subset\n",
    "    dataloader = DataLoader(subset_dataset, batch_size=64, sampler=sampler, num_workers=2)\n",
    " \n",
    "    # Model\n",
    "    model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "    model.to(device)\n",
    "    model = DDP(model, device_ids=[local_rank])\n",
    " \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    " \n",
    "    # --- Start Benchmarking ---\n",
    "    if local_rank == 0:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        start = time.time()\n",
    " \n",
    "    for epoch in range(2): \n",
    "        model.train()\n",
    "        sampler.set_epoch(epoch)\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    " \n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    " \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    " \n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    " \n",
    "        if local_rank == 0:\n",
    "            acc = correct / total\n",
    "            print(f\"[GPU {local_rank}] Epoch {epoch+1} | Loss: {total_loss/total:.4f} | Acc: {acc:.4f}\")\n",
    " \n",
    "    # --- End Benchmarking ---\n",
    "    if local_rank == 0:\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "        print(\"\\n--- Benchmark (DDP) ---\")\n",
    "        print(f\"Time: {duration:.2f} s\")\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"Max GPU Memory: {mem:.2f} GB\")\n",
    " \n",
    "        with open(\"benchmark_log.txt\", \"a\") as f:\n",
    "            f.write(f\"[{datetime.now()}] DDP (No AMP) ({dist.get_world_size()} GPUs)\\n\")\n",
    "            f.write(f\"Time: {duration:.2f}s | Acc: {acc:.4f} | Mem: {mem:.2f} GB\\n\\n\")\n",
    " \n",
    "    if local_rank == 0:\n",
    "        torch.save(model.module.state_dict(), \"ddp_model.pth\")\n",
    "        print(\"Model saved to ddp_model.pth\")\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e585938f-1ca0-468a-969f-e401f2ba605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/malikireddy.k/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "[GPU 0] Epoch 1 | Loss: 0.1369 | Acc: 0.9442\n",
      "[GPU 0] Epoch 2 | Loss: 0.0208 | Acc: 0.9939\n",
      "\n",
      "--- Benchmark (DDP) ---\n",
      "Time: 192.53 s\n",
      "Accuracy: 0.9939\n",
      "Max GPU Memory: 1.74 GB\n",
      "Model saved to ddp_model.pth\n",
      "[rank0]:[W415 20:03:29.615438445 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc-per-node=1 ddp_resnet_train.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d93328-94e3-4437-b28a-b94a309d17aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0415 20:06:35.555000 3112862 torch/distributed/run.py:793] \n",
      "W0415 20:06:35.555000 3112862 torch/distributed/run.py:793] *****************************************\n",
      "W0415 20:06:35.555000 3112862 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0415 20:06:35.555000 3112862 torch/distributed/run.py:793] *****************************************\n",
      "/home/malikireddy.k/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/malikireddy.k/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "[GPU 0] Epoch 1 | Loss: 0.1766 | Acc: 0.9177\n",
      "[GPU 0] Epoch 2 | Loss: 0.0399 | Acc: 0.9862\n",
      "\n",
      "--- Benchmark (DDP) ---\n",
      "Time: 173.00 s\n",
      "Accuracy: 0.9862\n",
      "Max GPU Memory: 1.74 GB\n",
      "Model saved to ddp_model.pth\n",
      "[rank0]:[W415 20:09:43.374754369 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc-per-node=2 ddp_resnet_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a24f97d6-abc5-4380-9fee-dc15d9dca4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0415 20:09:52.767000 3113892 torch/distributed/run.py:793] \n",
      "W0415 20:09:52.767000 3113892 torch/distributed/run.py:793] *****************************************\n",
      "W0415 20:09:52.767000 3113892 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0415 20:09:52.767000 3113892 torch/distributed/run.py:793] *****************************************\n",
      "/home/malikireddy.k/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/malikireddy.k/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/malikireddy.k/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "[GPU 0] Epoch 1 | Loss: 0.1900 | Acc: 0.9197\n",
      "[GPU 0] Epoch 2 | Loss: 0.0403 | Acc: 0.9865\n",
      "\n",
      "--- Benchmark (DDP) ---\n",
      "Time: 174.27 s\n",
      "Accuracy: 0.9865\n",
      "Max GPU Memory: 1.74 GB\n",
      "Model saved to ddp_model.pth\n",
      "[rank0]:[W415 20:13:05.388082172 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc-per-node=3 ddp_resnet_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f458302-9156-4196-92a3-c5ef79341a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0415 20:19:55.130000 3116545 torch/distributed/run.py:793] \n",
      "W0415 20:19:55.130000 3116545 torch/distributed/run.py:793] *****************************************\n",
      "W0415 20:19:55.130000 3116545 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0415 20:19:55.130000 3116545 torch/distributed/run.py:793] *****************************************\n",
      "/home/malikireddy.k/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/malikireddy.k/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/malikireddy.k/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/malikireddy.k/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "[GPU 0] Epoch 1 | Loss: 0.2483 | Acc: 0.8784\n",
      "[GPU 0] Epoch 2 | Loss: 0.0572 | Acc: 0.9770\n",
      "\n",
      "--- Benchmark (DDP) ---\n",
      "Time: 179.13 s\n",
      "Accuracy: 0.9770\n",
      "Max GPU Memory: 1.74 GB\n",
      "Model saved to ddp_model.pth\n",
      "[rank0]:[W415 20:23:16.032105651 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc-per-node=4 ddp_resnet_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed506208-2988-4d53-a702-cda6602c0ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
